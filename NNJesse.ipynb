{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.eye(8)\n",
    "#Each row is a learning example\n",
    "\n",
    "Y = X\n",
    "print(Y)\n",
    "\n",
    "# We need to have 8 inputs (each number in a row of the matrix), a hidden layer with 3 nodes + bias and an output layer with 8 nodes\n",
    "\n",
    "n_input = 8 # We need to have 8 inputs (each number in a row of the matrix)\n",
    "n_hidden = 3\n",
    "n_output = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.96714153e-05 -1.38264301e-05  6.47688538e-05]\n",
      " [ 1.52302986e-04 -2.34153375e-05 -2.34136957e-05]\n",
      " [ 1.57921282e-04  7.67434729e-05 -4.69474386e-05]\n",
      " [ 5.42560044e-05 -4.63417693e-05 -4.65729754e-05]\n",
      " [ 2.41962272e-05 -1.91328024e-04 -1.72491783e-04]\n",
      " [-5.62287529e-05 -1.01283112e-04  3.14247333e-05]\n",
      " [-9.08024076e-05 -1.41230370e-04  1.46564877e-04]\n",
      " [-2.25776300e-05  6.75282047e-06 -1.42474819e-04]]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.01\n",
    "\n",
    "#Weights\n",
    "np.random.seed(42)\n",
    "\n",
    "w1 = np.random.normal(0, epsilon**2, (n_input, n_hidden)) # 8 x 3 Weights for input to hidden\n",
    "print(w1)\n",
    "w2 = np.random.normal(0, epsilon**2, (n_hidden, n_output)) # 3 x 8 Weights for hidden to output\n",
    "#print(w2)\n",
    "\n",
    "#Biases\n",
    "b1 = np.random.normal(0, epsilon**2, n_hidden) # Bias for hidden layer (3)\n",
    "b2 = np.random.normal(0, epsilon**2, n_output) # Bias for output layer (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation function, our single neuron corresponds exactly to the input-output mapping defined by logistic regression.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#Used to compute the weighted input for a layer, which then is put into the activation function\n",
    "def weighted_input(X, W, b):\n",
    "    return np.dot(X, W) + b\n",
    "\n",
    "#No regularization yet\n",
    "#One-half squared-error cost function\n",
    "def cost(Y, second_layer_output):\n",
    "    return np.mean(0.5 * (Y - second_layer_output) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Could also have used a*(1-a) if we don't want to calculate z serperatly\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "#Element-wise multiplication\n",
    "def delta_output(Y, a, z):\n",
    "    return (a-Y)*sigmoid_derivative(z)\n",
    "\n",
    "#In order to do the matrix multiplication, we use this order because each row is a learning example\n",
    "def delta_hidden(W, next_delta, z):\n",
    "    return np.dot(next_delta, np.transpose(W))*sigmoid_derivative(z)\n",
    "\n",
    "#In order to do the matrix multiplication, again we use this order because each row is a learning example\n",
    "def gradient_weight(next_delta, a):\n",
    "    return np.dot(np.transpose(a), next_delta)\n",
    "    \n",
    "#Sum over rows (axis=0), as again each row is a learning example\n",
    "def gradient_bias(next_delta):\n",
    "    return np.sum(next_delta, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 0.1249971171330625\n",
      "Loss after iteration 100 : 0.05999217289289761\n",
      "Loss after iteration 200 : 0.06248039442734494\n",
      "Loss after iteration 300 : 0.06249990535973206\n",
      "Loss after iteration 400 : 0.06249999967287157\n",
      "Loss after iteration 500 : 0.062499999999122355\n",
      "Loss after iteration 600 : 0.06249999999999803\n",
      "Loss after iteration 700 : 0.0625\n",
      "Loss after iteration 800 : 0.0625\n",
      "Loss after iteration 900 : 0.0625\n"
     ]
    }
   ],
   "source": [
    "Delta_w1 = 0\n",
    "Delta_w2 = 0\n",
    "Delta_b1 = 0\n",
    "Delta_b2 = 0\n",
    "\n",
    "alpha = 0.001\n",
    "\n",
    "for i in range(1000):\n",
    "    #Hidden layer\n",
    "    z2 = weighted_input(X, w1, b1)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    #Output layer\n",
    "    z3 = weighted_input(a2, w2, b2)\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    #Output layer\n",
    "    delta3 = delta_output(Y, a3, z3)\n",
    "\n",
    "    #Hidden layer\n",
    "    delta2 = delta_hidden(w2, delta3, z2)\n",
    "    \n",
    "    #Gradient for weight is same shape as W1/W2 (8x3/3x8)\n",
    "    #Gradient for bias is same shape as b1/b2 (1x3/1x8)\n",
    "    Delta_w2 += gradient_weight(delta3, a2)\n",
    "    Delta_b2 += gradient_bias(delta3)\n",
    "\n",
    "    Delta_w1 += gradient_weight(delta2, X)\n",
    "    Delta_b1 += gradient_bias(delta2)\n",
    "\n",
    "    #Update weights, without regularization term\n",
    "    w1 += -alpha*np.mean(Delta_w1)\n",
    "    w2 += -alpha*np.mean(Delta_w2)\n",
    "\n",
    "    #Update bias\n",
    "    b1 += -alpha*np.mean(Delta_b1)\n",
    "    b2 += -alpha*np.mean(Delta_b2)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "            print(\"Loss after iteration\", i, \":\", cost(Y, a3))\n",
    "             \n",
    "            # print(w1)\n",
    "            # print(w2)\n",
    "            \n",
    "    \n",
    "           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
