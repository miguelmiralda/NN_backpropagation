{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.eye(8)\n",
    "#Each row is a learning example\n",
    "\n",
    "Y = X\n",
    "print(Y)\n",
    "\n",
    "# We need to have 8 inputs (each number in a row of the matrix), a hidden layer with 3 nodes + bias and an output layer with 8 nodes\n",
    "\n",
    "n_input = 8 # We need to have 8 inputs (each number in a row of the matrix)\n",
    "n_hidden = 3\n",
    "n_output = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00496714 -0.00138264  0.00647689]\n",
      " [ 0.0152303  -0.00234153 -0.00234137]\n",
      " [ 0.01579213  0.00767435 -0.00469474]\n",
      " [ 0.0054256  -0.00463418 -0.0046573 ]\n",
      " [ 0.00241962 -0.0191328  -0.01724918]\n",
      " [-0.00562288 -0.01012831  0.00314247]\n",
      " [-0.00908024 -0.01412304  0.01465649]\n",
      " [-0.00225776  0.00067528 -0.01424748]]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "#Weights\n",
    "np.random.seed(42)\n",
    "\n",
    "w1 = np.random.normal(0, epsilon**2, (n_input, n_hidden)) # 8 x 3 Weights for input to hidden\n",
    "print(w1)\n",
    "w2 = np.random.normal(0, epsilon**2, (n_hidden, n_output)) # 3 x 8 Weights for hidden to output\n",
    "#print(w2)\n",
    "\n",
    "#Biases\n",
    "b1 = np.random.normal(0, epsilon**2, n_hidden) # Bias for hidden layer (3)\n",
    "b2 = np.random.normal(0, epsilon**2, n_output) # Bias for output layer (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation function, our single neuron corresponds exactly to the input-output mapping defined by logistic regression.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#Used to compute the weighted input for a layer, which then is put into the activation function\n",
    "def weighted_input(X, W, b):\n",
    "    return np.dot(X, W) + b\n",
    "\n",
    "#No regularization yet\n",
    "#One-half squared-error cost function\n",
    "def cost(Y, second_layer_output):\n",
    "    return np.mean(0.5 * (Y - second_layer_output) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Could also have used a*(1-a) if we don't want to calculate z serperatly\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "#Element-wise multiplication\n",
    "def delta_output(Y, a, z):\n",
    "    return (a-Y)*sigmoid_derivative(z)\n",
    "\n",
    "#In order to do the matrix multiplication, we use this order because each row is a learning example\n",
    "def delta_hidden(W, next_delta, z):\n",
    "    return np.dot(next_delta, np.transpose(W))*sigmoid_derivative(z)\n",
    "\n",
    "#In order to do the matrix multiplication, again we use this order because each row is a learning example\n",
    "def gradient_weight(next_delta, a):\n",
    "    return np.dot(np.transpose(a), next_delta)\n",
    "    \n",
    "#Sum over rows (axis=0), as again each row is a learning example\n",
    "def gradient_bias(next_delta):\n",
    "    return np.sum(next_delta, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 0.12471918594172093\n",
      "[[0.49926481 0.49735264 0.50095755 0.50115934 0.49997571 0.49420559\n",
      "  0.4962487  0.50472465]\n",
      " [0.49925726 0.49735304 0.50095031 0.50116414 0.49997988 0.49420886\n",
      "  0.49624818 0.50473059]\n",
      " [0.4992559  0.4973462  0.50095522 0.50115707 0.49998315 0.49419755\n",
      "  0.49624033 0.50473092]\n",
      " [0.49925955 0.49735363 0.50095635 0.50116402 0.4999854  0.49421449\n",
      "  0.49625443 0.50471743]\n",
      " [0.49925488 0.49736165 0.50095197 0.50117674 0.49999628 0.49423846\n",
      "  0.49627122 0.50470385]\n",
      " [0.49926695 0.49735733 0.50096091 0.50116415 0.49998163 0.49421973\n",
      "  0.4962609  0.50470912]\n",
      " [0.49927348 0.49736096 0.50096051 0.50116422 0.49997176 0.49422007\n",
      "  0.4962622  0.50471223]\n",
      " [0.49925769 0.49734856 0.5009653  0.50115997 0.49999784 0.49421371\n",
      "  0.49625567 0.50470285]]\n",
      "Loss after iteration 1000 : 0.054680272743123266\n",
      "[[0.12449936 0.12443852 0.12451027 0.12455369 0.12471104 0.1246134\n",
      "  0.12470286 0.12447258]\n",
      " [0.12449359 0.12454301 0.12453527 0.12465687 0.12481934 0.12466507\n",
      "  0.12469628 0.12456208]\n",
      " [0.1245654  0.12453733 0.12470935 0.12466531 0.12491292 0.124658\n",
      "  0.12469709 0.12463078]\n",
      " [0.12511874 0.12516395 0.12517386 0.12529425 0.12547672 0.12530026\n",
      "  0.12533127 0.12519021]\n",
      " [0.12573643 0.125791   0.12588029 0.12594416 0.12619785 0.12591164\n",
      "  0.1259161  0.12585952]\n",
      " [0.12520229 0.12519419 0.12518966 0.12532212 0.12546596 0.12537547\n",
      "  0.12544903 0.12520028]\n",
      " [0.12521201 0.12514595 0.12514712 0.12527134 0.12538706 0.12536693\n",
      "  0.12548084 0.12514127]\n",
      " [0.12466964 0.12469864 0.12477457 0.12482339 0.12503647 0.12481277\n",
      "  0.1248367  0.12475582]]\n",
      "Loss after iteration 2000 : 0.05452081813787486\n",
      "[[0.12523924 0.12350949 0.12520923 0.12367395 0.12451533 0.12489914\n",
      "  0.12732672 0.12350166]\n",
      " [0.12388622 0.12474123 0.12455767 0.12483639 0.12584477 0.12484973\n",
      "  0.12586901 0.12429946]\n",
      " [0.12488539 0.12392915 0.12796953 0.12435454 0.12738834 0.12396413\n",
      "  0.12512997 0.12533532]\n",
      " [0.12545842 0.12627165 0.12639453 0.12649294 0.12778461 0.12642382\n",
      "  0.12740581 0.1259857 ]\n",
      " [0.12672926 0.12778381 0.13001167 0.12830917 0.13139331 0.12715102\n",
      "  0.12736754 0.12867997]\n",
      " [0.12649625 0.12608207 0.12578455 0.1262331  0.12648922 0.12732507\n",
      "  0.12936831 0.12529745]\n",
      " [0.12781569 0.12598562 0.12590645 0.12613218 0.12573414 0.12825245\n",
      "  0.1313259  0.12500681]\n",
      " [0.1240746  0.12452381 0.12623167 0.124783   0.12694865 0.12429163\n",
      "  0.12506972 0.12500477]]\n",
      "Loss after iteration 3000 : 0.04949890997372618\n",
      "[[0.15925502 0.11598733 0.15754036 0.11416257 0.11849408 0.13620684\n",
      "  0.20218534 0.1156256 ]\n",
      " [0.12090707 0.14343537 0.12678548 0.14020682 0.15257366 0.13885998\n",
      "  0.14957409 0.12848757]\n",
      " [0.14624657 0.11720483 0.23937589 0.1214564  0.20143051 0.11056929\n",
      "  0.12190691 0.14861515]\n",
      " [0.123415   0.14640459 0.13571046 0.14446427 0.16429658 0.14028613\n",
      "  0.14833396 0.13435131]\n",
      " [0.11650456 0.14448491 0.2039677  0.14913817 0.25168725 0.11561445\n",
      "  0.09417791 0.16566256]\n",
      " [0.1432256  0.14168586 0.11983044 0.13761365 0.12340296 0.1594673\n",
      "  0.21246431 0.11935409]\n",
      " [0.19041309 0.13866519 0.12427212 0.13516081 0.09674763 0.19349326\n",
      "  0.33572356 0.1121245 ]\n",
      " [0.12147316 0.13125267 0.16697057 0.13129861 0.18112582 0.11861179\n",
      "  0.11854103 0.13823514]]\n",
      "Loss after iteration 4000 : 0.03200967281280653\n",
      "[[0.44763446 0.04427047 0.21086    0.0403483  0.01456488 0.07328016\n",
      "  0.16664165 0.09348002]\n",
      " [0.06558925 0.27771523 0.01147581 0.25240668 0.10530071 0.2431397\n",
      "  0.06478263 0.13063855]\n",
      " [0.31563311 0.04064269 0.70410356 0.0534242  0.20250797 0.04014743\n",
      "  0.03765226 0.15846606]\n",
      " [0.06185251 0.27344783 0.0180363  0.2641558  0.1586115  0.22725652\n",
      "  0.05758976 0.1426819 ]\n",
      " [0.05451441 0.1579902  0.15934188 0.18297929 0.66561652 0.07493281\n",
      "  0.00352207 0.2144641 ]\n",
      " [0.10082532 0.24386585 0.01033749 0.21102969 0.0403295  0.26543892\n",
      "  0.15752164 0.10898534]\n",
      " [0.30665615 0.1323414  0.04239085 0.12358337 0.00788374 0.25792703\n",
      "  0.7258045  0.08283288]\n",
      " [0.12147321 0.12596724 0.07495665 0.12158448 0.15150816 0.10069009\n",
      "  0.02168006 0.14283964]]\n",
      "Loss after iteration 5000 : 0.022506772721106936\n",
      "[[6.97094761e-01 1.17772339e-02 1.40414907e-01 4.50009562e-03\n",
      "  1.40634166e-03 3.76232770e-02 1.16396899e-01 1.44132701e-01]\n",
      " [1.75105838e-02 3.53709497e-01 1.83370291e-03 2.93753204e-01\n",
      "  4.43891576e-02 3.24729275e-01 2.92951647e-02 1.44949608e-01]\n",
      " [2.22352755e-01 1.23099940e-02 8.05680301e-01 2.84517137e-02\n",
      "  1.62320770e-01 8.30434484e-03 1.57177305e-02 1.81913430e-01]\n",
      " [8.29309513e-03 3.28784916e-01 1.07621246e-02 4.83685353e-01\n",
      "  2.26035051e-01 1.96051351e-01 2.52321151e-02 1.42897561e-01]\n",
      " [2.00088984e-02 9.79646571e-02 1.12089066e-01 1.64157705e-01\n",
      "  7.21198787e-01 2.58379586e-02 4.25034691e-04 2.76934867e-01]\n",
      " [4.54255869e-02 3.10797724e-01 8.55557361e-04 1.65269549e-01\n",
      "  5.22969370e-03 4.50848789e-01 1.29287153e-01 1.17017833e-01]\n",
      " [2.23071547e-01 8.57423151e-02 1.59527723e-02 6.07470313e-02\n",
      "  8.29653506e-04 2.64057558e-01 8.05455978e-01 7.20414880e-02]\n",
      " [8.29810986e-02 6.99107684e-02 4.25038806e-02 5.47506228e-02\n",
      "  1.14368867e-01 4.74513259e-02 2.73467205e-03 2.26542187e-01]]\n",
      "Loss after iteration 6000 : 0.013391422355062573\n",
      "[[7.98076002e-01 3.48398336e-03 1.03518079e-01 7.69023726e-04\n",
      "  4.65952914e-04 2.82005091e-02 9.59644535e-02 2.06136956e-01]\n",
      " [5.05290322e-03 5.18062407e-01 1.29167305e-03 2.52444669e-01\n",
      "  7.84912522e-02 2.81820682e-01 8.70845271e-03 1.62248735e-01]\n",
      " [1.49142053e-01 5.73077923e-03 8.48783649e-01 3.10109242e-02\n",
      "  1.40075986e-01 1.70496323e-03 1.11521808e-02 1.27402356e-01]\n",
      " [4.25345373e-03 1.93976793e-01 6.05784381e-02 6.45103670e-01\n",
      "  1.36509060e-01 5.52934624e-02 8.59705972e-02 4.66061490e-02]\n",
      " [9.59821088e-03 1.56547843e-01 8.65650418e-02 1.01423421e-01\n",
      "  7.89687937e-01 9.31343955e-03 1.48017192e-04 3.40540133e-01]\n",
      " [3.84185443e-02 3.14882865e-01 3.17826599e-04 5.23739506e-02\n",
      "  1.33243399e-03 6.32376835e-01 1.40132397e-01 1.29464526e-01]\n",
      " [1.34192235e-01 3.80608024e-02 1.61167996e-02 6.09614661e-02\n",
      "  4.05717948e-04 2.20120896e-01 7.99269031e-01 4.04316234e-02]\n",
      " [9.13431691e-02 8.60472821e-02 1.20811737e-02 7.24438563e-03\n",
      "  7.97283686e-02 3.98243944e-02 5.41783983e-04 4.46638165e-01]]\n",
      "Loss after iteration 7000 : 0.007247939187045305\n",
      "[[8.33235447e-01 4.56294109e-04 8.12646630e-02 3.24020141e-04\n",
      "  2.55935228e-04 2.73057512e-02 8.32762781e-02 1.88949194e-01]\n",
      " [1.32712826e-03 6.99362596e-01 7.58357299e-04 1.79940785e-01\n",
      "  1.33572748e-01 2.01534999e-01 3.03302082e-03 1.20260951e-01]\n",
      " [1.05728234e-01 1.85662923e-03 8.57972134e-01 4.06556015e-02\n",
      "  1.27817989e-01 5.44125391e-04 8.74489252e-03 4.45769414e-02]\n",
      " [2.69987158e-03 1.30465482e-01 8.78611038e-02 7.48148995e-01\n",
      "  8.21400528e-02 2.01299733e-02 1.21678119e-01 9.35914259e-03]\n",
      " [4.52615066e-03 1.45582077e-01 8.42966444e-02 5.37038659e-02\n",
      "  8.07901510e-01 2.92965717e-03 8.93073851e-05 2.23046835e-01]\n",
      " [3.28899305e-02 2.04744450e-01 1.67300465e-04 1.61804769e-02\n",
      "  5.59985656e-04 7.25618315e-01 1.34826446e-01 1.18706372e-01]\n",
      " [8.92674404e-02 1.48333090e-02 1.51702781e-02 8.95677465e-02\n",
      "  3.08382830e-04 1.88861199e-01 7.88272206e-01 1.51868546e-02]\n",
      " [1.10361404e-01 5.21323465e-02 2.07589726e-03 4.29387407e-04\n",
      "  2.99135461e-02 6.66032397e-02 2.03935699e-04 6.73611959e-01]]\n",
      "Loss after iteration 8000 : 0.0047337078576919385\n",
      "[[8.58250195e-01 1.05346401e-04 7.07168612e-02 1.74267320e-04\n",
      "  1.63487313e-04 2.25290588e-02 7.86832221e-02 1.43901495e-01]\n",
      " [5.33429181e-04 7.80196923e-01 3.94309661e-04 1.34791745e-01\n",
      "  1.25327633e-01 1.55994017e-01 1.64825114e-03 8.79696756e-02]\n",
      " [9.06333914e-02 7.60975546e-04 8.69256641e-01 4.44473888e-02\n",
      "  1.21519576e-01 2.07543105e-04 6.87352127e-03 1.95134879e-02]\n",
      " [1.76694598e-03 1.08411090e-01 7.86370151e-02 8.00403709e-01\n",
      "  5.92351959e-02 1.02441821e-02 1.16185354e-01 3.27880158e-03]\n",
      " [2.72809084e-03 1.10714496e-01 8.09665620e-02 3.37653470e-02\n",
      "  8.30343971e-01 1.01473806e-03 4.21626896e-05 1.64534104e-01]\n",
      " [2.43286943e-02 1.55072725e-01 7.95422057e-05 7.37561925e-03\n",
      "  2.91346234e-04 7.75663292e-01 1.11569988e-01 1.02905672e-01]\n",
      " [7.74567407e-02 7.32662346e-03 1.22063112e-02 9.37130505e-02\n",
      "  1.76839880e-04 1.64468060e-01 8.09626609e-01 6.61971642e-03]\n",
      " [9.28786159e-02 2.97335106e-02 8.57723078e-04 8.76315357e-05\n",
      "  2.06216425e-02 6.86418254e-02 9.20818214e-05 7.65639880e-01]]\n",
      "Loss after iteration 9000 : 0.003483912740551859\n",
      "[[8.76153282e-01 3.66435363e-05 6.51800123e-02 1.02567499e-04\n",
      "  1.10736048e-04 1.80425273e-02 7.36839432e-02 1.17566453e-01]\n",
      " [2.76381817e-04 8.20479557e-01 2.39325818e-04 1.11637345e-01\n",
      "  1.10638267e-01 1.28715815e-01 1.04574066e-03 6.75339926e-02]\n",
      " [8.20188508e-02 3.86215515e-04 8.81434072e-01 4.47218000e-02\n",
      "  1.11064478e-01 9.62355016e-05 5.45408476e-03 1.09541237e-02]\n",
      " [1.25099818e-03 9.51619085e-02 6.93257948e-02 8.28741889e-01\n",
      "  4.49198771e-02 6.25383171e-03 1.06933350e-01 1.60623476e-03]\n",
      " [1.78938776e-03 9.18775493e-02 7.31688051e-02 2.36223838e-02\n",
      "  8.52315290e-01 4.63856450e-04 1.91264605e-05 1.37898184e-01]\n",
      " [1.89742843e-02 1.27101360e-01 4.33979464e-05 4.06804809e-03\n",
      "  1.72109285e-04 8.08532759e-01 9.52813765e-02 9.27249447e-02]\n",
      " [7.11847986e-02 4.33613202e-03 1.01518710e-02 8.89139532e-02\n",
      "  1.05145809e-04 1.45147298e-01 8.30265881e-01 3.71268338e-03]\n",
      " [7.85332807e-02 1.88166752e-02 4.90780270e-04 3.05615434e-05\n",
      "  1.60941285e-02 6.50466132e-02 4.91966451e-05 8.08399944e-01]]\n",
      "Loss after iteration 10000 : 0.0027417055678477117\n",
      "[[8.88771416e-01 1.63370219e-05 6.09890008e-02 6.53102119e-05\n",
      "  8.03125091e-05 1.45771612e-02 6.85873760e-02 1.01125101e-01]\n",
      " [1.66635995e-04 8.45301825e-01 1.60604436e-04 9.72789107e-02\n",
      "  9.86868702e-02 1.11063403e-01 7.23699611e-04 5.43251621e-02]\n",
      " [7.55513035e-02 2.25810285e-04 8.91525731e-01 4.37869215e-02\n",
      "  1.01328134e-01 5.14814929e-05 4.40847280e-03 7.08500115e-03]\n",
      " [9.38195573e-04 8.58943073e-02 6.22942743e-02 8.47510127e-01\n",
      "  3.56731091e-02 4.25459995e-03 9.86448822e-02 9.38668292e-04]\n",
      " [1.26196974e-03 7.95149623e-02 6.63751296e-02 1.79245592e-02\n",
      "  8.69069014e-01 2.52009841e-04 9.59081507e-06 1.21026048e-01]\n",
      " [1.55409378e-02 1.09260592e-01 2.64569636e-05 2.54317413e-03\n",
      "  1.13280944e-04 8.31276717e-01 8.40252780e-02 8.55914225e-02]\n",
      " [6.65133476e-02 2.89784516e-03 8.65709485e-03 8.26755202e-02\n",
      "  6.78072538e-05 1.30238307e-01 8.46578521e-01 2.41063476e-03]\n",
      " [6.85925881e-02 1.30458463e-02 3.20774214e-04 1.37893275e-05\n",
      "  1.33895861e-02 6.10254137e-02 2.93862430e-05 8.34135436e-01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alpha = 0.5\n",
    "\n",
    "for i in range(10001):\n",
    "    \n",
    "    #Input layer\n",
    "    a1 = X\n",
    "\n",
    "    #Hidden layer\n",
    "    z2 = weighted_input(X, w1, b1)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    #Output layer\n",
    "    z3 = weighted_input(a2, w2, b2)\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    #Output layer\n",
    "    delta3 = delta_output(Y, a3, z3)\n",
    "\n",
    "    #Hidden layer\n",
    "    delta2 = delta_hidden(w2, delta3, z2)\n",
    "    \n",
    "    #Gradient for weight is same shape as W1/W2 (8x3/3x8)\n",
    "    #Gradient for bias is same shape as b1/b2 (1x3/1x8)\n",
    "    Delta_w2 = gradient_weight(delta3, a2)\n",
    "    Delta_b2 = gradient_bias(delta3)\n",
    "\n",
    "    Delta_w1 = gradient_weight(delta2, X)\n",
    "    Delta_b1 = gradient_bias(delta2)\n",
    "\n",
    "    #Update weights, without regularization term\n",
    "    #Devide by amount of learning examples\n",
    "    w1 += -alpha*(Delta_w1/8)\n",
    "    w2 += -alpha*(Delta_w2/8)\n",
    "\n",
    "    #Update bias\n",
    "    b1 += -alpha*(Delta_b1/8)\n",
    "    b2 += -alpha*(Delta_b2/8)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "            print(\"Loss after iteration\", i, \":\", cost(Y, a3))\n",
    "             \n",
    "            # print(w1)\n",
    "            # print(w2)\n",
    "            print(a3)\n",
    "            \n",
    "          \n",
    "           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
