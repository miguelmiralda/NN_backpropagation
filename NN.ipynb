{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03f400d",
   "metadata": {},
   "source": [
    "# NN with backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4d2d94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 0 : [[0.04930263 0.87100017 0.73591582 0.83295882 0.73538161 0.46304904\n",
      "  0.64656094 0.51715703]\n",
      " [0.60454338 0.00615453 0.70068792 0.80172282 0.72468707 0.44085469\n",
      "  0.61544978 0.4832863 ]]\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 0 : [[0.5258476  0.83494169 0.05646848 0.74150613 0.58164012 0.24006663\n",
      "  0.42922685 0.28339418]\n",
      " [0.54077816 0.82687863 0.53489088 0.02086553 0.59044469 0.24441432\n",
      "  0.41772433 0.27280637]]\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 0 : [[0.3327985  0.75764808 0.47139635 0.66418382 0.13953857 0.11206628\n",
      "  0.21932923 0.12704762]\n",
      " [0.32157358 0.74799631 0.47419277 0.6671279  0.38010425 0.42649442\n",
      "  0.21238363 0.13314764]]\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 100 : [[7.38813915e-01 2.65954976e-02 1.13403927e-02 4.37755639e-02\n",
      "  1.09171464e-02 8.58455978e-03 3.86351280e-04 3.47497444e-04]\n",
      " [2.54449615e-02 2.63841940e-01 2.86138741e-02 1.64428177e-02\n",
      "  5.22263483e-02 1.83132722e-04 7.21616203e-04 4.12250621e-04]]\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 100 : [[3.26502341e-02 4.72728206e-02 4.63611357e-02 9.01608472e-03\n",
      "  3.42477233e-04 1.38319574e-02 1.07863610e-03 1.08733451e-03]\n",
      " [2.09495331e-02 8.80619827e-03 3.94078970e-04 4.73188066e-01\n",
      "  1.19966212e-02 3.34216354e-02 1.65138404e-04 1.76997789e-04]]\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 100 : [[0.03284758 0.12713107 0.00032111 0.0575691  0.10648678 0.00444313\n",
      "  0.00132504 0.00092541]\n",
      " [0.02307932 0.00099208 0.00947133 0.11170506 0.00483021 0.07538855\n",
      "  0.00068661 0.00116232]]\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 200 : [[7.11825520e-01 8.61957170e-03 1.19744725e-02 8.29961159e-03\n",
      "  3.35572361e-03 2.85545826e-03 3.49955717e-04 3.30041462e-04]\n",
      " [2.91865803e-02 3.30620612e-02 6.98849543e-03 5.18963431e-03\n",
      "  8.74234563e-03 4.60386971e-07 2.42133438e-04 1.31045241e-04]]\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 200 : [[4.19981676e-02 1.15920212e-02 2.32554316e-02 5.27297769e-04\n",
      "  4.76110419e-06 6.26283485e-03 3.97840015e-04 4.34046780e-04]\n",
      " [1.72557902e-02 1.08533109e-04 2.21203767e-06 4.55694938e-02\n",
      "  6.13002651e-03 6.42479719e-03 3.15785257e-05 3.46066526e-05]]\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 200 : [[2.24667345e-02 2.22079396e-02 1.64704049e-05 1.79216717e-02\n",
      "  2.57986944e-02 1.34846479e-03 8.86541941e-04 6.73481241e-04]\n",
      " [2.05451398e-02 2.45243936e-06 6.69604506e-03 2.63273511e-02\n",
      "  2.37683270e-03 1.95543405e-02 4.55562038e-04 7.67151007e-04]]\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 300 : [[6.67541620e-01 5.80867589e-03 1.22820591e-02 9.24855327e-04\n",
      "  6.96910222e-03 2.74101867e-03 4.25463920e-04 3.99031860e-04]\n",
      " [2.87687468e-02 1.46482038e-02 2.81550610e-03 3.55053030e-03\n",
      "  3.89904985e-03 4.48514743e-08 1.22340529e-04 6.77016975e-05]]\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 300 : [[4.29932446e-02 4.35995846e-03 1.50090326e-02 2.14547286e-04\n",
      "  5.16677590e-07 3.94945050e-03 2.03185033e-04 2.25263481e-04]\n",
      " [8.47549121e-03 3.15818037e-05 4.07904974e-07 1.70418714e-02\n",
      "  2.25756312e-03 2.63809120e-03 1.39795403e-05 1.52064930e-05]]\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 300 : [[3.34042783e-02 9.98314838e-03 4.07986518e-06 6.96658682e-03\n",
      "  1.49650991e-02 9.44419028e-04 6.29142392e-04 4.94268131e-04]\n",
      " [2.29192224e-02 2.27273239e-07 3.87808252e-03 1.19088728e-02\n",
      "  1.57503824e-03 1.04932134e-02 3.06420231e-04 5.02940872e-04]]\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 400 : [[1.88847018e-01 2.70318904e-03 2.71170596e-02 1.08035340e-05\n",
      "  1.85793983e-02 1.09951246e-02 1.25003649e-03 1.22934439e-03]\n",
      " [1.56560745e-02 8.15552787e-03 7.72744714e-04 2.34858905e-03\n",
      "  9.64112686e-04 4.33555447e-09 6.91944158e-05 3.91893469e-05]]\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 400 : [[2.64807184e-02 6.11951137e-04 1.81804941e-02 5.07370269e-04\n",
      "  7.09792532e-09 3.32506946e-03 6.67709552e-05 8.16166925e-05]\n",
      " [1.10502305e-04 1.47719988e-05 8.55662714e-08 1.28488305e-02\n",
      "  9.06848138e-04 1.07739614e-03 7.87863880e-06 8.64790219e-06]]\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 400 : [[4.02204085e-02 7.96233026e-03 1.48217352e-07 7.82022871e-03\n",
      "  1.34785625e-02 1.61173836e-04 3.29801366e-04 2.55476076e-04]\n",
      " [2.08225590e-02 3.58406039e-08 1.60845387e-03 9.83290714e-03\n",
      "  2.81813775e-04 8.70145586e-03 1.75045582e-04 2.92535981e-04]]\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 500 : [[3.84325954e-02 1.80342145e-03 1.45643286e-02 1.76186397e-06\n",
      "  1.02531581e-02 6.95041988e-03 1.13063505e-03 1.12366041e-03]\n",
      " [4.10019827e-03 5.23849000e-03 3.66484726e-04 1.48216869e-03\n",
      "  5.79249684e-04 5.25659142e-10 4.04088359e-05 2.30122512e-05]]\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 500 : [[5.79136522e-03 2.86667454e-04 9.39436164e-03 6.92922612e-04\n",
      "  4.39367502e-10 2.09667149e-03 2.96014665e-05 3.76789229e-05]\n",
      " [1.09857786e-06 9.40280678e-06 6.49962907e-08 1.24579498e-02\n",
      "  9.39441399e-04 5.95858562e-04 4.70060280e-06 5.13954357e-06]]\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 500 : [[1.02152937e-02 5.43890029e-03 1.10922096e-08 7.92028278e-03\n",
      "  8.88557025e-03 3.72285225e-05 1.76411691e-04 1.35317870e-04]\n",
      " [7.01047989e-03 1.12639528e-08 1.21619090e-03 7.73443660e-03\n",
      "  7.35564856e-05 7.31364125e-03 1.02824044e-04 1.73734884e-04]]\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 600 : [[1.86438236e-02 1.53008211e-03 8.84784529e-03 6.42525092e-07\n",
      "  6.18194050e-03 4.52806917e-03 9.10533626e-04 9.03647352e-04]\n",
      " [2.07165397e-03 3.93998186e-03 2.44838022e-04 1.19386611e-03\n",
      "  4.88507239e-04 1.63109944e-10 2.76334560e-05 1.57861939e-05]]\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 600 : [[2.83005527e-03 2.75669910e-04 5.86244936e-03 6.51692308e-04\n",
      "  1.24154531e-10 1.59451061e-03 1.94738523e-05 2.47899955e-05]\n",
      " [1.26041529e-07 7.58556210e-06 6.61241786e-08 9.92435009e-03\n",
      "  9.18319109e-04 4.91165281e-04 3.18873264e-06 3.45942774e-06]]\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 600 : [[5.27210916e-03 4.01667843e-03 3.35249687e-09 6.02833943e-03\n",
      "  5.93057063e-03 2.10598919e-05 1.27621991e-04 9.76752995e-05]\n",
      " [3.79343575e-03 6.12962413e-09 1.05211742e-03 5.99796427e-03\n",
      "  4.04667879e-05 5.39575946e-03 7.52522392e-05 1.26450472e-04]]\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 700 : [[1.19762880e-02 1.31685048e-03 6.15753979e-03 3.04865792e-07\n",
      "  4.28158382e-03 3.33105527e-03 7.53749742e-04 7.47406197e-04]\n",
      " [1.33066030e-03 3.19361201e-03 1.90432992e-04 1.03725935e-03\n",
      "  4.30657200e-04 7.06717566e-11 2.05674324e-05 1.17856262e-05]]\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 700 : [[1.82837889e-03 2.79754371e-04 4.22798895e-03 5.95229927e-04\n",
      "  5.32224440e-11 1.29484475e-03 1.43539477e-05 1.82112057e-05]\n",
      " [3.13565966e-08 6.99996352e-06 7.58580147e-08 7.98239210e-03\n",
      "  7.68422618e-04 3.98853143e-04 2.32158972e-06 2.49942457e-06]]\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 700 : [[3.52789883e-03 3.16687131e-03 1.55255759e-09 4.68601593e-03\n",
      "  4.38047521e-03 1.46084602e-05 1.01205255e-04 7.73966522e-05]\n",
      " [2.54560442e-03 3.88859937e-09 9.39593236e-04 4.80854353e-03\n",
      "  2.73374390e-05 4.19726933e-03 5.96631977e-05 9.96461813e-05]]\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 800 : [[8.72960760e-03 1.14767520e-03 4.65146422e-03 1.67648818e-07\n",
      "  3.23544822e-03 2.62699240e-03 6.40814023e-04 6.35176856e-04]\n",
      " [9.61674780e-04 2.69398379e-03 1.59907007e-04 9.30813679e-04\n",
      "  3.90224386e-04 3.67326403e-11 1.61408014e-05 9.27617405e-06]]\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 800 : [[1.33917644e-03 2.83245967e-04 3.30252350e-03 5.44929287e-04\n",
      "  2.81540295e-11 1.09206471e-03 1.12288375e-05 1.41975937e-05]\n",
      " [1.12700442e-08 6.86838022e-06 8.97738296e-08 6.60647251e-03\n",
      "  6.21446203e-04 3.23535851e-04 1.77698407e-06 1.90019057e-06]]\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 800 : [[2.64133269e-03 2.60053311e-03 8.80037255e-10 3.77931034e-03\n",
      "  3.44710587e-03 1.11654105e-05 8.39685496e-05 6.42196329e-05]\n",
      " [1.90049703e-03 2.67096200e-09 8.51904290e-04 3.96807231e-03\n",
      "  2.05689572e-05 3.41412875e-03 4.94512084e-05 8.21499440e-05]]\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 900 : [[6.83081154e-03 1.01285877e-03 3.70452744e-03 1.01635862e-07\n",
      "  2.58395115e-03 2.16364301e-03 5.56360994e-04 5.51379821e-04]\n",
      " [7.45567507e-04 2.33292593e-03 1.40305461e-04 8.49792316e-04\n",
      "  3.59884030e-04 2.14750141e-11 1.31416591e-05 7.57314117e-06]]\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 900 : [[1.05194388e-03 2.84095498e-04 2.71011701e-03 5.02616895e-04\n",
      "  1.69276064e-11 9.45498667e-04 9.13072420e-06 1.15094880e-05]\n",
      " [5.02895881e-09 6.92112483e-06 1.06132495e-07 5.60678809e-03\n",
      "  5.05441201e-04 2.65786576e-04 1.41219444e-06 1.50131546e-06]]\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 900 : [[2.10411454e-03 2.19861247e-03 5.61184851e-10 3.14064640e-03\n",
      "  2.82920416e-03 9.04926481e-06 7.17047924e-05 5.48687635e-05]\n",
      " [1.51041569e-03 1.93097767e-09 7.79968350e-04 3.35363554e-03\n",
      "  1.65365888e-05 2.86765886e-03 4.22100290e-05 6.97983321e-05]]\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = [[1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0],[0,0,1,0,0,0,0,0],[0,0,0,1,0,0,0,0],\n",
    "[0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1]]\n",
    "x_array = np.array(x)\n",
    "X_train = x_array[0:6]\n",
    "print(X_train)\n",
    "X_test = x_array[7:8]\n",
    "Y = x_array\n",
    "#print(Y)\n",
    "\n",
    "# We need to have 8 inputs (each number in a row of the matrix),a hidden layer with 3 nodes + bias and an output layer with 8 nodes\n",
    "\n",
    "n_input = 8 # We need to have 8 inputs (each number in a row of the matrix)\n",
    "n_hidden = 3\n",
    "n_output = 8\n",
    "# Weights\n",
    "np.random.seed(42)\n",
    "W1 = np.random.rand(n_input, n_hidden) # 8 x 3 Weights for input to hidden\n",
    "#print(W1)\n",
    "W2 = np.random.rand(n_hidden, n_output) # 3 x 8 Weights for hidden to output\n",
    "#print(W2)\n",
    "# Biases\n",
    "b1 = np.random.rand(n_hidden) # Bias for hidden layer (3)\n",
    "b2 = np.random.rand(n_output) # Bias for output layer (8)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def first_layer(X):\n",
    "    return sigmoid(np.dot(X, W1) + b1)\n",
    "\n",
    "def second_layer(hidden_output):\n",
    "    return sigmoid(np.dot(hidden_output, W2) + b2)\n",
    "\n",
    "second_layer_output_0 = second_layer(first_layer(X_train))\n",
    "#print(second_layer_output_0)\n",
    "# def sigmoid_derivative(x):\n",
    "#     return x * (1 - x)\n",
    "for i in range(1000): # Training loop\n",
    "    # Forward pass\n",
    "    batches = np.array_split(X_train, 3)\n",
    "    Y_batches = np.array_split(Y[0:6], 3)\n",
    "    for X_batch, Y_batch in zip(batches, Y_batches):\n",
    "        hidden_output = first_layer(X_batch)\n",
    "        output = second_layer(hidden_output)\n",
    "        #print(\"Output after iteration\", i, \":\", output)\n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss_value = ((Y_batch - output) ** 2)\n",
    "\n",
    "        \n",
    "        # Backpropagation\n",
    "        output_error = Y_batch - output\n",
    "        output_delta = output_error * (output * (1 - output))\n",
    "        \n",
    "        hidden_error = output_delta.dot(W2.T)\n",
    "        hidden_delta = hidden_error * (hidden_output * (1 - hidden_output))\n",
    "        \n",
    "        # Update weights and biases\n",
    "        W2 += hidden_output.T.dot(output_delta)\n",
    "        b2 += np.sum(output_delta, axis=0)\n",
    "        W1 += X_batch.T.dot(hidden_delta)\n",
    "        b1 += np.sum(hidden_delta, axis=0)\n",
    "        if i % 100 == 0:\n",
    "            print(\"Loss after iteration\", i, \":\", loss_value)\n",
    "            print (\"Batch:\", X_batch)\n",
    "\n",
    "    \n",
    "\n",
    "def loss(Y, second_layer_output_0):\n",
    "    return ((Y - second_layer_output_0) ** 2)\n",
    "#loss_value = loss(Y[0:6], second_layer_output_0)\n",
    "#print(\"Loss:\", loss_value[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bell_inequality)",
   "language": "python",
   "name": "bell_inequality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
