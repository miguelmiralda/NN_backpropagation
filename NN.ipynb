{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03f400d",
   "metadata": {},
   "source": [
    "# NN with backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d4d2d94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 0.28839726685116684\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 0 : 0.2583339066252565\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 0 : 0.24508647072899026\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 0 : 0.22708160581013812\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 1000 : 0.055617764576311415\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 1000 : 0.05149381567289357\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 1000 : 0.05415683907482446\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 1000 : 0.055791378884365264\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 2000 : 0.055786334374763186\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 2000 : 0.03936840904959244\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 2000 : 0.052095267264100596\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 2000 : 0.0567574954397565\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 3000 : 0.05396604216803613\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 3000 : 0.01964698979816797\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 3000 : 0.04455465119604168\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 3000 : 0.05614367497992594\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 4000 : 0.05010703438776249\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 4000 : 0.012510204393210506\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 4000 : 0.03232667471864496\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 4000 : 0.054361195283458806\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 5000 : 0.04233159083491498\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 5000 : 0.010161746965788452\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 5000 : 0.020336259701053856\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 5000 : 0.05106005488659823\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 6000 : 0.035280672066323536\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 6000 : 0.008318592659611192\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 6000 : 0.013403438533184328\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 6000 : 0.04542944068870408\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 7000 : 0.031843587091300855\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 7000 : 0.0068577442876432354\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 7000 : 0.010021589318845336\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 7000 : 0.038219257462307754\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 8000 : 0.029925660215150494\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 8000 : 0.005586676896067151\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 8000 : 0.007680115054620478\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 8000 : 0.03284968826902633\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 9000 : 0.02876011023317357\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 9000 : 0.004703533653788041\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 9000 : 0.00612952691398312\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 9000 : 0.03024506591525118\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 10000 : 0.0279865027084093\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 10000 : 0.004152846851633412\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 10000 : 0.005153554376981088\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 10000 : 0.02895275288799226\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 11000 : 0.027403570161287496\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 11000 : 0.0037977751545470086\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 11000 : 0.004509456750740443\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 11000 : 0.028220441325434324\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 12000 : 0.026895835256511746\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 12000 : 0.0035625120942952013\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 12000 : 0.004061760749088858\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 12000 : 0.027758016338301945\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 13000 : 0.02638521045266608\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 13000 : 0.00341218241929603\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 13000 : 0.0037392368897341635\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 13000 : 0.027438892275370514\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 14000 : 0.02580923580663644\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 14000 : 0.00333361982846124\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 14000 : 0.003502417052289298\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 14000 : 0.027197394904378006\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 15000 : 0.025104269017592183\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 15000 : 0.003327126020196604\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 15000 : 0.003327761636837278\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 15000 : 0.026995271755026095\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 16000 : 0.024179627457772135\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 16000 : 0.003403770388263657\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 16000 : 0.003200203066271101\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 16000 : 0.026812473910024892\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 17000 : 0.022867270355876823\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 17000 : 0.0035853813947883313\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 17000 : 0.003110648536990753\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 17000 : 0.026644986078802564\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 18000 : 0.02083357807339995\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 18000 : 0.0039016828938708997\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 18000 : 0.003060947375197482\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 18000 : 0.026488519369075977\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 19000 : 0.017586401492694335\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 19000 : 0.004341870921689618\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 19000 : 0.0030930757720805287\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 19000 : 0.026229508775081557\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 20000 : 0.01357624672744219\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 20000 : 0.004666082470416838\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 20000 : 0.0032764501240909664\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 20000 : 0.02535943122693926\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 21000 : 0.010378852855949347\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 21000 : 0.004685897117628052\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 21000 : 0.003680374306878482\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 21000 : 0.02352928824903174\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 22000 : 0.008135907981798217\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 22000 : 0.0046198879547359895\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 22000 : 0.0043246783768996966\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 22000 : 0.020839834803343726\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 23000 : 0.006608033104700335\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 23000 : 0.004539937423909015\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 23000 : 0.004960679656094446\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 23000 : 0.017255246773840525\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 24000 : 0.005608701103825172\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 24000 : 0.004325366217824223\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 24000 : 0.005081072475462808\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 24000 : 0.013159953412167623\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 25000 : 0.00492888133111673\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 25000 : 0.003910751616657412\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 25000 : 0.004500644636691109\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 25000 : 0.00955577025072952\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 26000 : 0.004388028719665212\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 26000 : 0.0034201107909931023\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 26000 : 0.0036843201502416096\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 26000 : 0.00699706821512921\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Loss after iteration 27000 : 0.003908795848464506\n",
      "Batch: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "Loss after iteration 27000 : 0.0029655281601933202\n",
      "Batch: [[0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "Loss after iteration 27000 : 0.0030031563138809706\n",
      "Batch: [[0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n",
      "Loss after iteration 27000 : 0.005344617121759647\n",
      "Batch: [[0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Final predictions on all training data:\n",
      "[[7.97798756e-01 8.92722284e-02 1.24322631e-01 7.91995257e-05\n",
      "  1.23630926e-03 2.28007804e-04 8.47247077e-04 7.10640353e-02]\n",
      " [1.41143856e-01 8.29641309e-01 5.11293217e-03 4.81778408e-02\n",
      "  1.58627644e-02 2.23752565e-05 1.21256985e-02 1.75180052e-04]\n",
      " [6.20372041e-02 1.81886925e-03 8.73590083e-01 1.46204358e-02\n",
      "  6.64483271e-06 7.60474503e-02 4.92138613e-03 2.01406955e-03]\n",
      " [1.82829772e-03 1.34558650e-01 1.15445423e-02 8.45809772e-01\n",
      "  1.73066357e-03 2.54225696e-03 1.48838592e-01 8.36878455e-06]\n",
      " [1.94086029e-02 2.77966035e-02 5.20518742e-06 5.16045041e-04\n",
      "  8.88089190e-01 2.49217550e-04 1.64961879e-01 5.47491838e-02]\n",
      " [1.07297983e-03 4.25029486e-06 2.86986146e-02 3.57722038e-03\n",
      "  4.16757795e-04 8.73403825e-01 1.71816405e-01 5.14095462e-02]]\n",
      "Predictions on test data:\n",
      "[[1.50737544e-04 5.16170097e-04 7.22935254e-05 1.43557104e-01\n",
      "  1.07594975e-01 8.16813932e-02 7.10434936e-01 1.09569096e-03]\n",
      " [1.11977745e-01 5.94587334e-05 2.40755727e-03 4.53584379e-06\n",
      "  2.19021745e-02 5.26298242e-02 1.72189981e-02 8.95015166e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = [[1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0],[0,0,1,0,0,0,0,0],[0,0,0,1,0,0,0,0],\n",
    "[0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1]]\n",
    "x_array = np.array(x)\n",
    "X_train = x_array[0:6]\n",
    "#print(X_train)\n",
    "X_test = x_array[6:8]\n",
    "Y = x_array\n",
    "#print(Y)\n",
    "\n",
    "# We need to have 8 inputs (each number in a row of the matrix),a hidden layer with 3 nodes + bias and an output layer with 8 nodes\n",
    "\n",
    "n_input = 8 # We need to have 8 inputs (each number in a row of the matrix)\n",
    "n_hidden = 3\n",
    "n_output = 8\n",
    "# Weights\n",
    "np.random.seed(42)\n",
    "W1 = np.random.rand(n_input, n_hidden) # 8 x 3 Weights for input to hidden\n",
    "#print(W1)\n",
    "W2 = np.random.rand(n_hidden, n_output) # 3 x 8 Weights for hidden to output\n",
    "#print(W2)\n",
    "# Biases\n",
    "b1 = np.random.rand(n_hidden) # Bias for hidden layer (3)\n",
    "b2 = np.random.rand(n_output) # Bias for output layer (8)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def first_layer(X, W1=W1, b1=b1):\n",
    "    return sigmoid(np.dot(X, W1) + b1)\n",
    "#print(first_layer(X_train))\n",
    "\n",
    "def second_layer(hidden_output, W2=W2, b2=b2):\n",
    "    return sigmoid(np.dot(hidden_output, W2) + b2)\n",
    "\n",
    "second_layer_output_0 = second_layer(first_layer(X_train))\n",
    "#print(second_layer_output_0)\n",
    "# def sigmoid_derivative(x):\n",
    "#     return x * (1 - x)\n",
    "for i in range(100000): # Training loop\n",
    "    # Forward pass\n",
    "    batches = np.array_split(x_array, 4)\n",
    "    Y_batches = np.array_split(Y[0:8], 4)\n",
    "    for X_batch, Y_batch in zip(batches, Y_batches):\n",
    "        hidden_output = first_layer(X_batch)\n",
    "        #print(\"Hidden output after iteration\", i, \":\", hidden_output)\n",
    "        output = second_layer(hidden_output)\n",
    "        #print(\"Output after iteration\", i, \":\", output)\n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss_value = (1/2 *(Y_batch - output) ** 2)\n",
    "\n",
    "        \n",
    "        # Backpropagation\n",
    "        output_error = Y_batch - output # This is the result of doing the derivative of the loss function; \n",
    "        # The minus sign of gradient descent has already been taken into account here\n",
    "        output_delta = output_error * (output * (1 - output)) # loss derivative * sigmoid derivative (descent direction)\n",
    "        #print(\"Output delta after iteration\", i, \":\", output_delta) # 8 x 2 matrix\n",
    "        \n",
    "        hidden_error = output_delta.dot(W2.T) # 3 x 2 matrix\n",
    "        #print(\"Hidden error after iteration\", i, \":\", hidden_error)\n",
    "        hidden_delta = hidden_error * (hidden_output * (1 - hidden_output))\n",
    "        lr = 0.02 # Learning rate\n",
    "        # Update weights and biases\n",
    "        W2 += lr*hidden_output.T.dot(output_delta)\n",
    "        b2 += np.sum(output_delta, axis=0)\n",
    "        W1 += lr* X_batch.T.dot(hidden_delta)\n",
    "        b1 += np.sum(hidden_delta, axis=0)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Loss after iteration\", i, \":\", np.mean(loss_value))\n",
    "            print (\"Batch:\", X_batch)\n",
    "    if np.mean(loss_value) < 0.005:\n",
    "        final_output = second_layer(first_layer(X_train))\n",
    "        print(\"Final predictions on all training data:\")\n",
    "        print(final_output)\n",
    "        print(\"Predictions on test data:\")\n",
    "        print(second_layer(first_layer(X_test)))\n",
    "\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "def loss(Y, second_layer_output_0):\n",
    "    return (1/2*(Y - second_layer_output_0) ** 2)\n",
    "#loss_value = loss(Y[0:6], second_layer_output_0)\n",
    "#print(\"Loss:\", loss_value[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bell_inequality)",
   "language": "python",
   "name": "bell_inequality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
