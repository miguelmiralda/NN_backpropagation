{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03f400d",
   "metadata": {},
   "source": [
    "# NN with backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4d2d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = [[1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0],[0,0,1,0,0,0,0,0],[0,0,0,1,0,0,0,0],\n",
    "[0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1]]\n",
    "x_array = np.array(x)\n",
    "X_train = x_array[0:6]\n",
    "#print(X_train)\n",
    "X_test = x_array[7:8]\n",
    "Y = x_array\n",
    "#print(Y)\n",
    "\n",
    "# We need to have 8 inputs (each number in a row of the matrix),a hidden layer with 3 nodes + bias and an output layer with 8 nodes\n",
    "\n",
    "n_input = 8 # We need to have 8 inputs (each number in a row of the matrix)\n",
    "n_hidden = 3\n",
    "n_output = 8\n",
    "# Weights\n",
    "np.random.seed(42)\n",
    "W1 = np.random.rand(n_input, n_hidden) # 8 x 3 Weights for input to hidden\n",
    "#print(W1)\n",
    "W2 = np.random.rand(n_hidden, n_output) # 3 x 8 Weights for hidden to output\n",
    "#print(W2)\n",
    "# Biases\n",
    "b1 = np.random.rand(n_hidden) # Bias for hidden layer (3)\n",
    "b2 = np.random.rand(n_output) # Bias for output layer (8)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def first_layer(X):\n",
    "    return sigmoid(np.dot(X, W1) + b1)\n",
    "\n",
    "def second_layer(hidden_output):\n",
    "    return sigmoid(np.dot(hidden_output, W2) + b2)\n",
    "\n",
    "second_layer_output_0 = second_layer(first_layer(X_train))\n",
    "#print(second_layer_output_0)\n",
    "# def sigmoid_derivative(x):\n",
    "#     return x * (1 - x)\n",
    "for i in range(1000): # Training loop\n",
    "    # Forward pass\n",
    "    batches = np.array_split(X_train, 3)\n",
    "    Y_batches = np.array_split(Y[0:6], 3)\n",
    "    for X_batch, Y_batch in zip(batches, Y_batches):\n",
    "        hidden_output = first_layer(X_batch)\n",
    "        output = second_layer(hidden_output)\n",
    "        #print(\"Output after iteration\", i, \":\", output)\n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss_value = ((Y_batch - output) ** 2)\n",
    "\n",
    "        \n",
    "        # Backpropagation\n",
    "        output_error = Y_batch - output\n",
    "        output_delta = output_error * (output * (1 - output))\n",
    "        \n",
    "        hidden_error = output_delta.dot(W2.T)\n",
    "        hidden_delta = hidden_error * (hidden_output * (1 - hidden_output))\n",
    "        \n",
    "        # Update weights and biases\n",
    "        W2 += hidden_output.T.dot(output_delta)\n",
    "        b2 += np.sum(output_delta, axis=0)\n",
    "        W1 += X_batch.T.dot(hidden_delta)\n",
    "        b1 += np.sum(hidden_delta, axis=0)\n",
    "        # if i % 100 == 0:\n",
    "        #     print(\"Loss after iteration\", i, \":\", loss_value)\n",
    "        #     print (\"Batch:\", X_batch)\n",
    "\n",
    "    \n",
    "\n",
    "def loss(Y, second_layer_output_0):\n",
    "    return ((Y - second_layer_output_0) ** 2)\n",
    "#loss_value = loss(Y[0:6], second_layer_output_0)\n",
    "#print(\"Loss:\", loss_value[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bell_inequality)",
   "language": "python",
   "name": "bell_inequality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
